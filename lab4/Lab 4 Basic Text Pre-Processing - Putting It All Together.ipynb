{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42746c02-99d4-43aa-ac6f-3fcc333d8e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5676\\119513715.py:65: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0  Im happy with uniten actually, even the people...   \n",
      "1  I’m having a pretty good time here, happy to m...   \n",
      "2        a very neutral place in terms of everything   \n",
      "3  I would say Uniten it's  a good university  bu...   \n",
      "4   UNITEN is well-regarded, particularly for its...   \n",
      "\n",
      "                                           processed  \n",
      "0     [im, happy, united, actually, even, people, w]  \n",
      "1  [i, ’, m, pretty, good, time, happy, meet, w, ...  \n",
      "2                 [neutral, place, term, everything]  \n",
      "3  [would, say, united, good, university, issue, ...  \n",
      "4  [united, wellregarded, particularly, strong, e...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from googletrans import Translator\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize tools\n",
    "spell = Speller(lang='en')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "translator = Translator()\n",
    "\n",
    "# Exclude \"UNITEN\" from stopword removal\n",
    "custom_exceptions = {\"uniten\",\"UNITEN\",\"Uniten\"}\n",
    "\n",
    "# Function to translate text to English\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        translated = translator.translate(text, dest='en')\n",
    "        return translated.text\n",
    "    except Exception as e:\n",
    "        print(f\"Translation Error: {e}\")\n",
    "        return text  # Return original text if translation fails\n",
    "\n",
    "# Function to remove stopwords but keep \"UNITEN\"\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words or word in custom_exceptions]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Function to remove numbers from text\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = text.split()  # Tokenize text into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatize each word\n",
    "    return ' '.join(lemmatized_words)  # Join words back into a sentence\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML\n",
    "    text = emoji.replace_emoji(text, replace='')  # Remove emojis\n",
    "    text = remove_punctuation(text)  # ✅ Now defined!\n",
    "    text = remove_numbers(text)  # ✅ Now defined!\n",
    "    text = spell(text)\n",
    "    text = remove_stopwords(text)  # Remove stopwords (keeping \"UNITEN\")\n",
    "    text = lemmatize_text(text)  # ✅ Now defined!\n",
    "    text = word_tokenize(text)  # Tokenization\n",
    "    return text\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"UNITENReview.csv\")  # Replace with your actual dataset\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"processed\"] = df[\"Review\"].apply(preprocess_text)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv(\"Processed_Reviews2.csv\", index=False)\n",
    "\n",
    "# Display results\n",
    "print(df[[\"Review\", \"processed\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4303e0c3-72b8-4635-8cdf-7bc5b1b862c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd703191-32d0-4854-bf59-9c4d1cd41631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic:\n",
      "                                                 Article  Topic\n",
      "0      In the Washington of 2016, even when the polic...      2\n",
      "1        Donald Trump has used Twitter  —   his prefe...      2\n",
      "2        Donald Trump is unabashedly praising Russian...      2\n",
      "3      Updated at 2:50 p. m. ET, Russian President Vl...      2\n",
      "4      From photography, illustration and video, to d...      3\n",
      "...                                                  ...    ...\n",
      "11987  The number of law enforcement officers shot an...      0\n",
      "11988    Trump is busy these days with victory tours,...      2\n",
      "11989  It’s always interesting for the Goats and Soda...      1\n",
      "11990  The election of Donald Trump was a surprise to...      2\n",
      "11991  Voters in the English city of Sunderland did s...      4\n",
      "\n",
      "[11992 rows x 2 columns]\n",
      "\n",
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"state\" (weight: 0.012)\n",
      "- \"law\" (weight: 0.010)\n",
      "- \"school\" (weight: 0.009)\n",
      "- \"court\" (weight: 0.007)\n",
      "- \"student\" (weight: 0.007)\n",
      "- \"federal\" (weight: 0.006)\n",
      "- \"case\" (weight: 0.006)\n",
      "- \"police\" (weight: 0.005)\n",
      "- \"right\" (weight: 0.005)\n",
      "- \"department\" (weight: 0.005)\n",
      "\n",
      "Topic 1:\n",
      "- \"health\" (weight: 0.007)\n",
      "- \"study\" (weight: 0.005)\n",
      "- \"percent\" (weight: 0.005)\n",
      "- \"food\" (weight: 0.005)\n",
      "- \"company\" (weight: 0.004)\n",
      "- \"care\" (weight: 0.004)\n",
      "- \"child\" (weight: 0.003)\n",
      "- \"patient\" (weight: 0.003)\n",
      "- \"need\" (weight: 0.003)\n",
      "- \"university\" (weight: 0.003)\n",
      "\n",
      "Topic 2:\n",
      "- \"trump\" (weight: 0.030)\n",
      "- \"clinton\" (weight: 0.012)\n",
      "- \"president\" (weight: 0.011)\n",
      "- \"republican\" (weight: 0.008)\n",
      "- \"campaign\" (weight: 0.008)\n",
      "- \"state\" (weight: 0.007)\n",
      "- \"election\" (weight: 0.006)\n",
      "- \"obama\" (weight: 0.006)\n",
      "- \"vote\" (weight: 0.005)\n",
      "- \"voter\" (weight: 0.005)\n",
      "\n",
      "Topic 3:\n",
      "- \"know\" (weight: 0.005)\n",
      "- \"think\" (weight: 0.005)\n",
      "- \"thing\" (weight: 0.005)\n",
      "- \"life\" (weight: 0.005)\n",
      "- \"really\" (weight: 0.004)\n",
      "- \"woman\" (weight: 0.004)\n",
      "- \"story\" (weight: 0.004)\n",
      "- \"show\" (weight: 0.003)\n",
      "- \"book\" (weight: 0.003)\n",
      "- \"u\" (weight: 0.003)\n",
      "\n",
      "Topic 4:\n",
      "- \"country\" (weight: 0.007)\n",
      "- \"city\" (weight: 0.005)\n",
      "- \"report\" (weight: 0.005)\n",
      "- \"government\" (weight: 0.004)\n",
      "- \"attack\" (weight: 0.004)\n",
      "- \"war\" (weight: 0.004)\n",
      "- \"day\" (weight: 0.004)\n",
      "- \"two\" (weight: 0.004)\n",
      "- \"police\" (weight: 0.004)\n",
      "- \"last\" (weight: 0.003)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Download NLTK Resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load the Data\n",
    "df = pd.read_csv('npr.csv')\n",
    "documents = df['Article'].tolist()\n",
    "\n",
    "# Preprocess the Data\n",
    "stop_words = set(stopwords.words('english'))  # Create a set of English stopwords\n",
    "lemmatizer = WordNetLemmatizer()  # Initialize a WordNet lemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize the text into words and convert to lowercase\n",
    "    tokens = [token for token in tokens if token.isalnum()]  # Filter out non-alphanumeric tokens\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords from the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize each token\n",
    "    return tokens  # Return the preprocessed tokens\n",
    "\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]  # Preprocess each document\n",
    "\n",
    "# Create document-term matrix\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)  # Filter out tokens that appear in less than 15 documents or more than 50% of the documents\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]  # Convert each preprocessed document into a bag-of-words representation using the dictionary\n",
    "\n",
    "# Run LDA\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)  # Train an LDA model on the corpus with 5 topics\n",
    "\n",
    "# Interpret Results\n",
    "article_labels = []  # Empty list to store dominant topic labels for each document\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    "    bow = dictionary.doc2bow(doc)  # Convert to bag-of-words representation\n",
    "    topics = lda_model.get_document_topics(bow)  # Get list of topic probabilities\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]  # Determine topic with highest probability\n",
    "    article_labels.append(dominant_topic)  # Append to the list\n",
    "\n",
    "df_result = pd.DataFrame({\"Article\": documents, \"Topic\": article_labels})  # Create DataFrame\n",
    "print(\"Table with Articles and Topic:\")\n",
    "print(df_result)\n",
    "print()\n",
    "\n",
    "# Print top terms for each topic\n",
    "print(\"Top Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
